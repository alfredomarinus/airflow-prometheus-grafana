x-airflow-common: &airflow-common
  image: apache/airflow:3.0.2
  environment: &airflow-common-env
    # Core Configuration
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__TEST_CONNECTION: 'Enabled'

    # Authentication Configuration (FAB Auth Manager)
    AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.providers.fab.auth_manager.api.auth.backend.basic_auth,airflow.providers.fab.auth_manager.api.auth.backend.session'
    AIRFLOW__API__EXPOSE_CONFIG: ${AIRFLOW__API__EXPOSE_CONFIG}
    
    # FAB Auth Manager Configuration
    AIRFLOW__FAB__AUTH_TYPE: 'AUTH_DB'
    AIRFLOW__FAB__AUTH_ROLE_ADMIN: 'Admin'
    AIRFLOW__FAB__AUTH_ROLE_PUBLIC: 'Public'
    
    # FAB Database Configuration - Prevent duplicate role creation
    AIRFLOW__FAB__AUTH_ROLE_SYNC_AT_LOGIN: 'false'
    AIRFLOW__FAB__AUTH_USER_REGISTRATION: 'false'
    AIRFLOW__FAB__AUTH_USER_REGISTRATION_ROLE: 'Public'

    # Logging Configuration
    AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW__LOGGING__REMOTE_LOGGING}
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: ${AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID:-minio_conn}
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: ${AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER:-s3://airflow-logs}
    AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: ${AIRFLOW__LOGGING__ENCRYPT_S3_LOGS}
    AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
    AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARN
    AIRFLOW__LOGGING__COLORED_CONSOLE_LOG: 'false'

    # Cleaned up expired sessions
    AIRFLOW__API__SESSION_LIFETIME_SECONDS: 86400
    AIRFLOW__API__CHECK_SESSION_PERMANENT: true
    
    # Timezone Configuration
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW__CORE__DEFAULT_TIMEZONE}
    AIRFLOW__API__DEFAULT_UI_TIMEZONE: ${AIRFLOW__API__DEFAULT_UI_TIMEZONE}
    
    # External Connections
    AIRFLOW_CONN_MINIO_CONN: ${AIRFLOW_CONN_MINIO_CONN:-}
    CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP: ${CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP:-true}
    
    # Security Configuration
    AIRFLOW__API__CSRF_ENABLED: 'true'
    AIRFLOW__CORE__SENSITIVE_VAR_CONN_NAMES: 'AIRFLOW_CONN_.*,AIRFLOW_VAR_.*'
    AIRFLOW__API__COOKIE_SECURE: 'false'
    AIRFLOW__API__COOKIE_SAMESITE: 'Lax'
    
    # Performance Configuration
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM:-32}
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: ${AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG:-16}
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG:-16}
    AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: ${AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT:-30}
    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'false'
    AIRFLOW__CORE__KILLED_TASK_CLEANUP_TIME: '60'
    
    # DAG Configuration
    AIRFLOW__CORE__DAGS_FOLDER: '/opt/airflow/dags'
    AIRFLOW__CORE__STORE_DAG_CODE: 'true'
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: 30
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL: 10
    
    # Scheduler Configuration  
    AIRFLOW__SCHEDULER__PARSING_PROCESSES: ${AIRFLOW__SCHEDULER__PARSING_PROCESSES:-2}
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: ${AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL:-30}
    AIRFLOW__SCHEDULER__USE_FILE_WATCHER: 'true'
    AIRFLOW__SCHEDULER__FILE_PARSING_SORT_MODE: 'modified_time'
    AIRFLOW__SCHEDULER__DAG_DISCOVERY_SAFE_MODE: 'false'
    AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION: 'true'
    AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP: ${AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP:-10}
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
    AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 512
    AIRFLOW__SCHEDULER__PROCESSOR_POLL_INTERVAL: '1'
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: '300'
    
    # API Server Configuration (Airflow 3.0 specific)
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_HEADERS: '*'
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_METHODS: '*'
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_ORIGINS: '*'
    AIRFLOW__API__ENABLE_EXPERIMENTAL_API: 'true'
    
    # Celery Configuration
    AIRFLOW__CELERY__WORKER_CONCURRENCY: ${AIRFLOW__CELERY__WORKER_CONCURRENCY:-16}
    AIRFLOW__CELERY__FLOWER_HOST: 0.0.0.0
    AIRFLOW__CELERY__FLOWER_PORT: 5555
    AIRFLOW__CELERY__OPERATION_TIMEOUT: 1.0
    
    # Metrics Configuration
    AIRFLOW__METRICS__STATSD_ON: ${AIRFLOW__METRICS__STATSD_ON:-false}
    AIRFLOW__METRICS__STATSD_HOST: ${AIRFLOW__METRICS__STATSD_HOST:-statsd-exporter}
    AIRFLOW__METRICS__STATSD_PORT: ${AIRFLOW__METRICS__STATSD_PORT:-9125}
    AIRFLOW__METRICS__STATSD_PREFIX: ${AIRFLOW__METRICS__STATSD_PREFIX:-airflow}
    
    # # Rate Limit Backend
    AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI: redis://redis:6379

    # Additional packages
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}

    # Additional security configurations
    AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: 'true'
    AIRFLOW__API__MAXIMUM_PAGE_LIMIT: '100'
    AIRFLOW__API__FALLBACK_PAGE_LIMIT: '100'
    
    # Enhanced logging
    AIRFLOW__LOGGING__ENABLE_TASK_CONTEXT_LOGGER: 'true'
    AIRFLOW__LOGGING__TASK_LOG_PREFIX_TEMPLATE: '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-./airflow}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-./airflow}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-./airflow}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-./airflow}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
      _AIRFLOW_WWW_USER_EMAIL: ${_AIRFLOW_WWW_USER_EMAIL}
      _AIRFLOW_WWW_USER_FIRSTNAME: ${_AIRFLOW_WWW_USER_FIRSTNAME}
      _AIRFLOW_WWW_USER_LASTNAME: ${_AIRFLOW_WWW_USER_LASTNAME}
      _AIRFLOW_WWW_USER_ROLE: ${_AIRFLOW_WWW_USER_ROLE}
    volumes:
      - ${AIRFLOW_PROJ_DIR}:/sources
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
          export AIRFLOW_UID=$(id -u)
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        echo
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
    restart: "no"
    depends_on:
      postgres:
        condition: service_healthy
    user: "0:0"
    networks:
      - data-stack

  airflow-api-server:
    <<: *airflow-common
    container_name: airflow-api-server
    ports:
      - "${AIRFLOW_API_SERVER_PORT}:8080"
    environment:
      <<: *airflow-common-env
      AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI: ${AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI}
    command: airflow api-server
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    environment:
      <<: *airflow-common-env
    command: scheduler
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      # test: ["CMD-SHELL", "airflow scheduler health || exit 1"]
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - data-stack
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    command: celery worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      # test: ["CMD-SHELL", "pgrep -f 'airflow celery worker' || exit 1"]
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: always
    networks:
      - data-stack
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    environment:
      <<: *airflow-common-env
    command: triggerer
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      # test: ["CMD-SHELL", "airflow triggerer health || exit 1"]
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: always
    networks:
      - data-stack
  
  airflow-dag-processor:
    <<: *airflow-common
    container_name: airflow-dag-processor
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - data-stack
    
  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command:
      - bash
      - -c
      - airflow
    depends_on:
      <<: *airflow-common-depends-on

  postgres:
    image: postgres:14.12
    container_name: postgres
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - data-stack

  flower:
    image: mher/flower:2.0.1
    container_name: flower
    depends_on:
      redis:
        condition: service_healthy
    env_file:
      - .env
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - FLOWER_PORT=5555
      - FLOWER_BASIC_AUTH=${FLOWER_USERNAME}:${FLOWER_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://${FLOWER_USERNAME}:${FLOWER_PASSWORD}@127.0.0.1:5555 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - data-stack

  redis:
    image: redis:7.2.5-alpine3.19
    container_name: redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - data-stack

  prometheus:
    image: prom/prometheus:v3.4.1
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - data-stack

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    networks:
      - data-stack

  minio:
    image: quay.io/minio/minio:RELEASE.2025-05-24T17-08-30Z
    container_name: minio
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "9002:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:9001/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - data-stack
  
  statsd-exporter:
    image: prom/statsd-exporter:v0.26.0
    container_name: statsd-exporter
    ports:
      - "9102:9102"
    volumes:
      - ./prometheus/airflow_statsd_mapping.yml:/etc/statsd/exporter.yml:ro
    command:
      - --statsd.mapping-config=/etc/statsd/exporter.yml
      - --web.listen-address=:9102
      - --statsd.listen-udp=:9125
    networks:
      - data-stack

volumes:
  postgres-db-volume:
  redis-data:
  minio-data:
  grafana-storage:

networks:
  data-stack:
    driver: bridge
    name: data-stack