x-airflow-common: &airflow-common
  image: apache/airflow:3.0.2
  environment: &airflow-common-env
    # Core Configuration
    AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__TEST_CONNECTION: 'Enabled'

    # Authentication Configuration (FAB Auth Manager)
    AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.providers.fab.auth_manager.api.auth.backend.basic_auth,airflow.providers.fab.auth_manager.api.auth.backend.session'
    AIRFLOW__API__EXPOSE_CONFIG: ${AIRFLOW__API__EXPOSE_CONFIG}
    
    # FAB Auth Manager Configuration
    AIRFLOW__FAB__AUTH_TYPE: 'AUTH_DB'
    AIRFLOW__FAB__AUTH_ROLE_ADMIN: 'Admin'
    AIRFLOW__FAB__AUTH_ROLE_PUBLIC: 'Public'
    
    # FAB Database Configuration - Prevent duplicate role creation
    AIRFLOW__FAB__AUTH_ROLE_SYNC_AT_LOGIN: 'false'
    AIRFLOW__FAB__AUTH_USER_REGISTRATION: 'false'
    AIRFLOW__FAB__AUTH_USER_REGISTRATION_ROLE: 'Public'

    # Logging Configuration
    AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW__LOGGING__REMOTE_LOGGING}
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: ${AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID:-minio_conn}
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: ${AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER:-s3://airflow-logs}
    AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: ${AIRFLOW__LOGGING__ENCRYPT_S3_LOGS}
    AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
    AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARN
    AIRFLOW__LOGGING__COLORED_CONSOLE_LOG: 'false'

    # Cleaned up expired sessions
    AIRFLOW__API__SESSION_LIFETIME_SECONDS: 86400
    AIRFLOW__API__CHECK_SESSION_PERMANENT: true
    
    # Timezone Configuration
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ${AIRFLOW__CORE__DEFAULT_TIMEZONE}
    AIRFLOW__API__DEFAULT_UI_TIMEZONE: ${AIRFLOW__API__DEFAULT_UI_TIMEZONE}
    
    # External Connections
    AIRFLOW_CONN_MINIO_CONN: ${AIRFLOW_CONN_MINIO_CONN:-}
    CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP: ${CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP:-true}
    
    # Security Configuration
    AIRFLOW__API__CSRF_ENABLED: 'true'
    AIRFLOW__CORE__SENSITIVE_VAR_CONN_NAMES: 'AIRFLOW_CONN_.*,AIRFLOW_VAR_.*'
    AIRFLOW__API__COOKIE_SECURE: 'false'
    AIRFLOW__API__COOKIE_SAMESITE: 'Lax'
    
    # Performance Configuration
    AIRFLOW__CORE__PARALLELISM: ${AIRFLOW__CORE__PARALLELISM:-32}
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: ${AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG:-16}
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: ${AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG:-16}
    AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: ${AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT:-30}
    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'false'
    AIRFLOW__CORE__KILLED_TASK_CLEANUP_TIME: '60'
    
    # DAG Configuration
    AIRFLOW__CORE__DAGS_FOLDER: '/opt/airflow/dags'
    AIRFLOW__CORE__STORE_DAG_CODE: 'true'
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: 30
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL: 10
    
    # Scheduler Configuration  
    AIRFLOW__SCHEDULER__PARSING_PROCESSES: ${AIRFLOW__SCHEDULER__PARSING_PROCESSES:-2}
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: ${AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL:-30}
    AIRFLOW__SCHEDULER__USE_FILE_WATCHER: 'true'
    AIRFLOW__SCHEDULER__FILE_PARSING_SORT_MODE: 'modified_time'
    AIRFLOW__SCHEDULER__DAG_DISCOVERY_SAFE_MODE: 'false'
    AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION: 'true'
    AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP: ${AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP:-10}
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
    AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 512
    AIRFLOW__SCHEDULER__PROCESSOR_POLL_INTERVAL: '1'
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: '300'
    
    # API Server Configuration (Airflow 3.0 specific)
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_HEADERS: '*'
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_METHODS: '*'
    AIRFLOW__API__ACCESS_CONTROL_ALLOW_ORIGINS: '*'
    AIRFLOW__API__ENABLE_EXPERIMENTAL_API: 'true'
    
    # Celery Configuration
    AIRFLOW__CELERY__WORKER_CONCURRENCY: ${AIRFLOW__CELERY__WORKER_CONCURRENCY:-16}
    AIRFLOW__CELERY__FLOWER_HOST: 0.0.0.0
    AIRFLOW__CELERY__FLOWER_PORT: 5555
    AIRFLOW__CELERY__OPERATION_TIMEOUT: 1.0
    
    # Metrics Configuration
    AIRFLOW__METRICS__STATSD_ON: ${AIRFLOW__METRICS__STATSD_ON:-false}
    AIRFLOW__METRICS__STATSD_HOST: ${AIRFLOW__METRICS__STATSD_HOST:-statsd-exporter}
    AIRFLOW__METRICS__STATSD_PORT: ${AIRFLOW__METRICS__STATSD_PORT:-9125}
    AIRFLOW__METRICS__STATSD_PREFIX: ${AIRFLOW__METRICS__STATSD_PREFIX:-airflow}
    
    # # Rate Limit Backend
    AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI: redis://redis:6379

    # Additional packages
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}

    # Additional security configurations
    AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS: 'true'
    AIRFLOW__API__MAXIMUM_PAGE_LIMIT: '100'
    AIRFLOW__API__FALLBACK_PAGE_LIMIT: '100'
    
    # Enhanced logging
    AIRFLOW__LOGGING__ENABLE_TASK_CONTEXT_LOGGER: 'true'
    AIRFLOW__LOGGING__TASK_LOG_PREFIX_TEMPLATE: '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-./airflow}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-./airflow}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-./airflow}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-./airflow}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  # Database initialization service
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    environment:
      <<: *airflow-common-env
    volumes:
      
    command:
      - bash
      - -c
      - |
        airflow db migrate
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
    restart: "no"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  airflow-api-server:
    <<: *airflow-common
    container_name: airflow-api-server
    ports:
      - "${AIRFLOW_API_SERVER_PORT}:8080"
    environment:
      <<: *airflow-common-env
      AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI: ${AIRFLOW__FAB__RATE_LIMIT_STORAGE_URI}
    command: airflow api-server
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    environment:
      <<: *airflow-common-env
    volumes:
      
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    environment:
      <<: *airflow-common-env
    volumes:
      
    command: celery worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    environment:
      <<: *airflow-common-env
    volumes:
      
    command: airflow triggerer
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  postgres:
    image: postgres:14.12
    container_name: postgres
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - data-stack

  flower:
    image: mher/flower:2.0.1
    container_name: flower
    depends_on:
      redis:
        condition: service_healthy
    env_file:
      - .env
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - FLOWER_PORT=5555
      - FLOWER_BASIC_AUTH=${FLOWER_USERNAME}:${FLOWER_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://${FLOWER_USERNAME}:${FLOWER_PASSWORD}@127.0.0.1:5555 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - data-stack

  redis:
    image: redis:7.2.5-alpine3.19
    container_name: redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - data-stack

  prometheus:
    image: prom/prometheus:v3.4.1
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    networks:
      - data-stack

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    networks:
      - data-stack

  minio:
    image: quay.io/minio/minio:RELEASE.2025-05-24T17-08-30Z
    container_name: minio
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "9002:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:9001/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - data-stack
  
  statsd-exporter:
    image: prom/statsd-exporter:v0.26.0
    container_name: statsd-exporter
    ports:
      - "9102:9102"
    volumes:
      - ./prometheus/airflow_statsd_mapping.yml:/etc/statsd/exporter.yml:ro
    command:
      - --statsd.mapping-config=/etc/statsd/exporter.yml
      - --web.listen-address=:9102
      - --statsd.listen-udp=:9125
    networks:
      - data-stack

volumes:
  postgres-db-volume:
  redis-data:
  minio-data:
  grafana-storage:

networks:
  data-stack:
    driver: bridge
    name: data-stack